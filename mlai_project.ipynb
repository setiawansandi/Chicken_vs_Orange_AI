{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/setiawansandi/image_recognition/blob/main/mlai_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75tDtfgK4XQi"
      },
      "source": [
        "# Training a model (Orange vs Chicken wings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mm2yQYNGwye",
        "outputId": "c1d99c45-e596-4cda-f237-83a0f036451a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'image_recognition'...\n",
            "remote: Enumerating objects: 9722, done.\u001b[K\n",
            "remote: Counting objects: 100% (1572/1572), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1504/1504), done.\u001b[K\n",
            "remote: Total 9722 (delta 101), reused 1009 (delta 68), pack-reused 8150\u001b[K\n",
            "Receiving objects: 100% (9722/9722), 120.04 MiB | 29.00 MiB/s, done.\n",
            "Resolving deltas: 100% (1146/1146), done.\n",
            "Checking out files: 100% (8101/8101), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/setiawansandi/image_recognition.git\n",
        "!mkdir -p saved_model\n",
        "from tensorflow import keras\n",
        "from keras.layers import Conv2D, Input, MaxPooling2D,Flatten, Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import tensorflow as tf "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Vj37FQ9yy6H"
      },
      "outputs": [],
      "source": [
        "batch_size = 32 #64\n",
        "epoch = 50 #50\n",
        "img_size = (256, 256)\n",
        "lr = 1e-5\n",
        "loss_function = \"categorical_crossentropy\"\n",
        "opt = keras.optimizers.Adam(learning_rate=1e-5)\n",
        "train_dir = '/content/image_recognition/dataset/train'\n",
        "val_dir = '/content/image_recognition/dataset/valid'\n",
        "seed = 12973\n",
        "\n",
        "def model_1():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(256, 256, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "\t# compile model\n",
        "  model.compile(optimizer=opt, loss = loss_function, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def model_2():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(256, 256, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "\t# compile model\n",
        "  model.compile(optimizer=opt, loss = loss_function, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def model_3():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(256, 256, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  # dropout regularization (ignore certain nodes (prevent overfitting))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dense(3, activation='softmax')) # 3 classess so the final dense variable must be 3\n",
        "\t# compile model\n",
        "  model.compile(optimizer=opt, loss = loss_function, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def run_test(option):\n",
        "  # define model\n",
        "  if (option == 1):\n",
        "    model = model_1()\n",
        "  elif (option == 2):\n",
        "    model = model_2()\n",
        "  else: \n",
        "    model = model_3() \n",
        " \n",
        "  # create image data generators (data augmentation)\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1.0/255.0, \n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      rotation_range=30,\n",
        "      brightness_range=[0.5,1.5], \n",
        "      horizontal_flip=True)\n",
        " \n",
        "  val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        " \n",
        "  # start generating\n",
        "  train_it = train_datagen.flow_from_directory(\n",
        "      train_dir,\n",
        "      class_mode='categorical',\n",
        "      batch_size=batch_size,\n",
        "      target_size=img_size,\n",
        "      shuffle=True,\n",
        "      seed=seed)\n",
        " \n",
        "  val_it = val_datagen.flow_from_directory(\n",
        "      val_dir,\n",
        "      class_mode='categorical',\n",
        "      batch_size=batch_size,\n",
        "      target_size=img_size)\n",
        "  \n",
        "  # fit model (Trains the model for a fixed number of epochs (iterations on a dataset))\n",
        "  history = model.fit_generator(train_it, \n",
        "                                steps_per_epoch=len(train_it),\n",
        "                                validation_data=val_it,\n",
        "                                validation_steps=len(val_it),\n",
        "                                epochs=epoch,\n",
        "                                verbose=1)\n",
        "\t# evaluate model\n",
        "  _, acc = model.evaluate_generator(val_it, steps=len(val_it), verbose=2)\n",
        "  print('> %.3f' % (acc * 100.0))\n",
        "\t# learning curves\n",
        "  plot_graph(history)\n",
        "  return model\n",
        " \n",
        "def plot_graph(history):\n",
        "  # plot loss\n",
        "  plt.subplot(121) # shorthand subplot(2, 1, 1) -> row, col, plot.no [211,212]\n",
        "  plt.title('Cross Entropy Loss')\n",
        "  plt.plot(history.history['loss'], color='lightskyblue', label='train') #plt(y, fmt)\n",
        "  plt.plot(history.history['val_loss'], color='lawngreen', label='valid')\n",
        "  plt.legend(loc='upper right')\n",
        "  # plot accuracy\n",
        "  plt.subplot(122)\n",
        "  plt.title('Classification Accuracy')\n",
        "  plt.plot(history.history['accuracy'], color='lightskyblue', label='train')\n",
        "  plt.plot(history.history['val_accuracy'], color='lawngreen', label='valid')\n",
        "  plt.legend(loc='lower right')\n",
        "  # save plot to file\n",
        "  filename = sys.argv[0].split('/')[-1]\n",
        "  plt.savefig(filename + '_plot.png')\n",
        "  plt.close()\n",
        "\n",
        "def save_model(model):\n",
        "  tf.keras.models.save_model(model,'chicken_wings_vs_orange.hdf5')\n",
        "\n",
        "# entry point, run the test\n",
        "if __name__ == \"__main__\":\n",
        "  model = run_test(3)\n",
        "  save_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh1iASa_hoOl"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kJ45rMR0fy7"
      },
      "source": [
        "# Load model for prediction\n",
        "Using the trained model from above or a pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da_Lv7Yim7fU",
        "outputId": "5b45cb17-a0c0-45a8-e951-b59ecbfe5dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No model found. Downloading pre-trained model...\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "# %%bash -> to signify that the cell is running bash (or could just put ! at the start of every commands)\n",
        "%%bash\n",
        "if ! [[ -f \"/content/chicken_wings_vs_orange.hdf5\" ]]\n",
        "then\n",
        "    echo \"No model found. Downloading pre-trained model...\"\n",
        "    wget -cq https://www.dropbox.com/s/zkrejfxhdb3l6re/chicken_wings_vs_orange.hdf5\n",
        "    echo \"Download completed\"\n",
        "else\n",
        "    echo \"Using trained model.\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZhfJPJOFGENo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# loading a trained model\n",
        "trained_model = tf.keras.models.load_model('/content/chicken_wings_vs_orange.hdf5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtphANtbGO2s"
      },
      "source": [
        "# Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bODE_TMWGOYV"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageOps\n",
        "\n",
        "# take image + model as arguments and return prediction accuracies\n",
        "def import_and_predict(image_data, model):\n",
        "  size = (256,256)    \n",
        "  image = ImageOps.fit(image_data, size, Image.ANTIALIAS)\n",
        "  image = image.convert('RGB')\n",
        "  image = np.asarray(image)\n",
        "  image = (image.astype(np.float32) / 255.0)\n",
        "\n",
        "  img_reshape = image[np.newaxis,...]\n",
        "\n",
        "  prediction = model.predict(img_reshape)\n",
        "\n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOwPPQUowST2"
      },
      "source": [
        "# **Prediction**\n",
        "# **1. Upload image**\n",
        "Upload image to predict (can be one image or multiple images at once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CQwn53vUzhev"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow as tf "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hulVEL2lOqmy"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()\n",
        "# filename = next(iter(uploaded)) # get file name\n",
        "print('<==========================================================>')\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "  image = Image.open(filename) # image for processing\n",
        "  prediction = import_and_predict(image, trained_model)\n",
        "\n",
        "  # display image\n",
        "  img = mpimg.imread(filename)\n",
        "  imgplot = plt.imshow(img)\n",
        "  plt.show()  # show image  \n",
        "\n",
        "  if np.argmax(prediction) == 0:\n",
        "      predicted = \"chicken wings\"\n",
        "  elif np.argmax(prediction) == 1:\n",
        "      predicted = \"orange\"\n",
        "  else:\n",
        "      predicted = \"unknown\"\n",
        "  print('file: \"{name}\" is {predict}!'.format(name=filename, predict = predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH4754cO4Vtz"
      },
      "source": [
        "# **2. Live detection (Livecam)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QnJbRVob6Px"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript\n",
        "from PIL import Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp3Huk0Q7Pqy"
      },
      "outputs": [],
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def overlay_to_bytes(overlay_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  overlay_PIL = PIL.Image.fromarray(overlay_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format the text into png for return\n",
        "  overlay_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  overlay_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return overlay_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z4l8OWFX7oY"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid #724CF9';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this video</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, overlay):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, overlay))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZkewJauY-n4"
      },
      "outputs": [],
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "overlay = ''\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, overlay)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    original = js_to_image(js_reply[\"img\"])\n",
        "    frame = cv2.resize(original, (256, 256))\n",
        "\n",
        "    # capture image to use for prediction\n",
        "    cv2.imwrite(filename='predict_img.jpg', img=frame)\n",
        "    predict_image = Image.open('predict_img.jpg')\n",
        "\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    overlay_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # get the prediction's accuracy\n",
        "    prediction = import_and_predict(predict_image, trained_model)\n",
        "\n",
        "    if np.argmax(prediction) == 0:\n",
        "        predict= \"Chicken Wings - \" + str(f'{prediction.item(0):.3}')\n",
        "    elif np.argmax(prediction) == 1:\n",
        "        predict= \"Orange - \" + str(f'{prediction.item(1):.3}')\n",
        "    else:\n",
        "        predict= \"Unknown - \" + str(f'{prediction.item(2):.3}')\n",
        "    \n",
        "    #print(predict)\n",
        "    overlay_array = cv2.putText(overlay_array, predict, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (114, 76, 249), 2)\n",
        "    # cv2.putText(image, text, org, font, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]])\n",
        "\n",
        "    # convert to rgb value (0-255)\n",
        "    overlay_array[:,:,3] = (overlay_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of text into bytes\n",
        "    overlay_bytes = overlay_to_bytes(overlay_array)\n",
        "    # update text so next frame gets new overlay (updated)\n",
        "    overlay = overlay_bytes\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "75tDtfgK4XQi",
        "2kJ45rMR0fy7",
        "NtphANtbGO2s",
        "YH4754cO4Vtz"
      ],
      "name": "mlai_project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbDC5sccTaC5jYyjcyOdkT",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}